{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fd1a3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cce72575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yutingd/LatentSB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7dbf4fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_dir = \"/nfs/turbo/sph-jianghui1/yutingd/HCP/fALFF/\"\n",
    "task_dir = \"/nfs/turbo/sph-jianghui1/yutingd/HCP/WM_Contrasts_2bk-0bk/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58408c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/nfs/turbo/sph-jianghui1/yutingd/HCP\n"
     ]
    }
   ],
   "source": [
    "%cd /nfs/turbo/sph-jianghui1/yutingd/HCP/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03d46310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 714 matched subjects.\n",
      "Example IDs: ['100206', '100307', '100408', '100610', '101006', '101107', '101309', '102311', '102513', '102715']\n",
      "Train: 499, Val: 142, Test: 73\n",
      "Files written:\n",
      " - training_ids.txt\n",
      " - validation_ids.txt\n",
      " - test_ids.txt\n"
     ]
    }
   ],
   "source": [
    "rest_ids = {\n",
    "    f.split(\"_\")[0]\n",
    "    for f in os.listdir(rest_dir)\n",
    "    if f.endswith(\"_fALFF.nii\")\n",
    "}\n",
    "\n",
    "task_ids = {\n",
    "    f.split(\"_\")[0]\n",
    "    for f in os.listdir(task_dir)\n",
    "    if f.endswith(\"_2bk-0bk.nii\")\n",
    "}\n",
    "\n",
    "# Intersection = matched subjects in BOTH folders\n",
    "matched_ids = sorted(list(rest_ids.intersection(task_ids)))\n",
    "\n",
    "print(f\"Found {len(matched_ids)} matched subjects.\")\n",
    "print(\"Example IDs:\", matched_ids[:10])\n",
    "\n",
    "# -------------------------\n",
    "# 3. Train / Val / Test split (7 : 2 : 1)\n",
    "# -------------------------\n",
    "np.random.seed(42)\n",
    "matched_ids = np.array(matched_ids)\n",
    "np.random.shuffle(matched_ids)\n",
    "\n",
    "n = len(matched_ids)\n",
    "n_train = int(0.7 * n)\n",
    "n_val   = int(0.2 * n)\n",
    "n_test  = n - n_train - n_val\n",
    "\n",
    "training_ids   = matched_ids[:n_train]\n",
    "validation_ids = matched_ids[n_train : n_train + n_val]\n",
    "test_ids       = matched_ids[n_train + n_val :]\n",
    "\n",
    "print(f\"Train: {len(training_ids)}, Val: {len(validation_ids)}, Test: {len(test_ids)}\")\n",
    "\n",
    "# -------------------------\n",
    "# 4. Write to files\n",
    "# -------------------------\n",
    "with open(\"training_ids.txt\", \"w\") as f:\n",
    "    for sid in training_ids:\n",
    "        f.write(f\"{sid}\\n\")\n",
    "\n",
    "with open(\"validation_ids.txt\", \"w\") as f:\n",
    "    for sid in validation_ids:\n",
    "        f.write(f\"{sid}\\n\")\n",
    "\n",
    "with open(\"test_ids.txt\", \"w\") as f:\n",
    "    for sid in test_ids:\n",
    "        f.write(f\"{sid}\\n\")\n",
    "\n",
    "print(\"Files written:\")\n",
    "print(\" - training_ids.txt\")\n",
    "print(\" - validation_ids.txt\")\n",
    "print(\" - test_ids.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa4f8ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499 142 73\n"
     ]
    }
   ],
   "source": [
    "def load_ids(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        return [line.strip() for line in f.readlines()]\n",
    "\n",
    "train_ids = load_ids(\"training_ids.txt\")\n",
    "val_ids   = load_ids(\"validation_ids.txt\")\n",
    "test_ids  = load_ids(\"test_ids.txt\")\n",
    "\n",
    "print(len(train_ids), len(val_ids), len(test_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f641263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yutingd/LatentSB\n"
     ]
    }
   ],
   "source": [
    "%cd /home/yutingd/LatentSB/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "626f62e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "190ee817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/yutingd/LatentSB\n"
     ]
    }
   ],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "621e7eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ae8b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "# train_autoencoder.py — full working script\n",
    "###############################################################\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from models.ae_3d import Encoder3D, Decoder3D\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "encoder = Encoder3D(latent_dim=128).to(device)\n",
    "decoder = Decoder3D(latent_dim=128).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    list(encoder.parameters()) + list(decoder.parameters()),\n",
    "    lr=3e-5\n",
    ")\n",
    "\n",
    "def train_autoencoder(epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for rest, task in train_loader:\n",
    "            # combine rest + task so AE learns shared latent space\n",
    "            X = torch.cat([rest, task], dim=0).to(device)   # (2, 1, H, W, D)\n",
    "\n",
    "            z, feat_shape = encoder(X)\n",
    "            X_hat = decoder(z, feat_shape, X.shape)\n",
    "            loss = F.mse_loss(X_hat, X)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: loss = {total_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# run it\n",
    "train_autoencoder(epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5efb4811",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "HCPRestTaskDataset.__init__() got an unexpected keyword argument 'mode'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m lr = \u001b[32m1e-4\u001b[39m\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# ---- datasets & loaders ----\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m train_ds = \u001b[43mHCPRestTaskDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrest_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrest_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtask_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubject_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m3d\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m val_ds = HCPRestTaskDataset(\n\u001b[32m     25\u001b[39m     rest_dir=rest_dir,\n\u001b[32m     26\u001b[39m     task_dir=task_dir,\n\u001b[32m   (...)\u001b[39m\u001b[32m     30\u001b[39m     normalize=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     31\u001b[39m )\n\u001b[32m     33\u001b[39m train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mTypeError\u001b[39m: HCPRestTaskDataset.__init__() got an unexpected keyword argument 'mode'"
     ]
    }
   ],
   "source": [
    "rest_dir = \"/nfs/turbo/sph-jianghui1/yutingd/HCP/fALFF/\"\n",
    "task_dir = \"/nfs/turbo/sph-jianghui1/yutingd/HCP/WM_Contrasts_2bk-0bk/\"\n",
    "\n",
    "mask_path = None#\"/nfs/turbo/sph-jianghui1/yutingd/HCP/AAL_MNI_2mm.nii\" #None  # or \"/path/to/brain_mask.nii.gz\"\n",
    "\n",
    "    #train_ids = load_ids(\"training_ids.txt\")\n",
    "    #val_ids = load_ids(\"validation_ids.txt\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "latent_dim = 128\n",
    "batch_size = 1\n",
    "epochs = 50\n",
    "lr = 1e-4\n",
    "\n",
    "    # ---- datasets & loaders ----\n",
    "train_ds = HCPRestTaskDataset(\n",
    "    rest_dir=rest_dir,\n",
    "    task_dir=task_dir,\n",
    "    subject_ids=train_ids,\n",
    "    mode=\"3d\",\n",
    "    mask_path=mask_path,\n",
    "    normalize=None,\n",
    ")\n",
    "val_ds = HCPRestTaskDataset(\n",
    "    rest_dir=rest_dir,\n",
    "    task_dir=task_dir,\n",
    "    subject_ids=val_ids,\n",
    "    mode=\"3d\",\n",
    "    mask_path=mask_path,\n",
    "    normalize=None,\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb6737db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<hcp_rest_task.HCPRestTaskDataset at 0x15005a5667b0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88e02e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import nibabel as nib\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11612c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_dir = \"/nfs/turbo/sph-jianghui1/yutingd/HCP/fALFF/\"\n",
    "task_dir = \"/nfs/turbo/sph-jianghui1/yutingd/HCP/WM_Contrasts_2bk-0bk/\"\n",
    "\n",
    "mask_path = \"/nfs/turbo/sph-jianghui1/yutingd/HCP/AAL_MNI_2mm.nii\" #None  # or \"/path/to/brain_mask.nii.gz\"\n",
    "\n",
    "    #train_ids = load_ids(\"training_ids.txt\")\n",
    "    #val_ids = load_ids(\"validation_ids.txt\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "latent_dim = 128\n",
    "batch_size = 1\n",
    "epochs = 50\n",
    "lr = 1e-4\n",
    "\n",
    "    # ---- datasets & loaders ----\n",
    "train_ds = HCPRestTaskDataset(\n",
    "        rest_dir=rest_dir,\n",
    "        task_dir=task_dir,\n",
    "        subject_ids=train_ids,\n",
    "        mode=\"3d\",\n",
    "        mask_path=mask_path,\n",
    "        normalize=None,\n",
    "    )\n",
    "val_ds = HCPRestTaskDataset(\n",
    "        rest_dir=rest_dir,\n",
    "        task_dir=task_dir,\n",
    "        subject_ids=val_ids,\n",
    "        mode=\"3d\",\n",
    "        mask_path=mask_path,\n",
    "        normalize=None,\n",
    "    )\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # ---- models ----\n",
    "encoder = Encoder3D(latent_dim=latent_dim).to(device)\n",
    "decoder = Decoder3D(latent_dim=latent_dim).to(device)\n",
    "\n",
    "\n",
    "for rest, task in tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs} [train]\"):\n",
    "            # combine rest + task to train a shared AE\n",
    "    X = torch.cat([rest, task], dim=0).to(device)  # (2,1,H,W,D)\n",
    "\n",
    "    z, feat_shape = encoder(X)\n",
    "    X_hat = decod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5ff6ad",
   "metadata": {},
   "source": [
    "# AE Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d8d6b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ===========================================\n",
    "# 1. Dataset with NaN cleaning + AAL mask\n",
    "# ===========================================\n",
    "\n",
    "class HCPRestTaskDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        rest_dir,\n",
    "        task_dir,\n",
    "        subject_ids,\n",
    "        mask_np=None,\n",
    "        normalize=\"zscore\",\n",
    "        global_mean=None, global_std=None\n",
    "    ):\n",
    "        self.rest_dir = rest_dir\n",
    "        self.task_dir = task_dir\n",
    "        self.subjects = list(subject_ids)\n",
    "        self.normalize = normalize\n",
    "        self.global_mean = global_mean\n",
    "        self.global_std = global_std\n",
    "\n",
    "\n",
    "        # broadcast mask to (1,1,H,W,D) later\n",
    "        self.mask_np = mask_np.astype(np.float32) if mask_np is not None else None\n",
    "\n",
    "        print(f\"[Dataset] Loaded {len(self.subjects)} subjects\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subjects)\n",
    "\n",
    "    def _clean(self, img):\n",
    "        \"\"\"Remove NaN, +Inf, -Inf.\"\"\"\n",
    "        return np.nan_to_num(img, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "#     def _normalize(self, img):\n",
    "#         img = self._clean(img)\n",
    "#         m, s = img.mean(), img.std()\n",
    "#         if s < 1e-6:\n",
    "#             return np.zeros_like(img)\n",
    "#         return (img - m) / s\n",
    "    \n",
    "    def _normalize(self, img):\n",
    "        img = self._clean(img)\n",
    "        if self.global_mean is None:\n",
    "            m, s = img.mean(), img.std()\n",
    "        else:\n",
    "            m, s = self.global_mean, self.global_std\n",
    "        if s < 1e-6:\n",
    "            return np.zeros_like(img)\n",
    "        return (img - m) / s\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sid = self.subjects[idx]\n",
    "\n",
    "        rest = nib.load(os.path.join(self.rest_dir, f\"{sid}_fALFF.nii\")).get_fdata()\n",
    "        task = nib.load(os.path.join(self.task_dir, f\"{sid}_2bk-0bk.nii\")).get_fdata()\n",
    "\n",
    "        rest = self._clean(rest)\n",
    "        task = self._clean(task)\n",
    "\n",
    "        if self.mask_np is not None:\n",
    "            rest = rest * self.mask_np\n",
    "            task = task * self.mask_np\n",
    "\n",
    "        rest = self._normalize(rest)\n",
    "        task = self._normalize(task)\n",
    "\n",
    "        rest = torch.tensor(rest, dtype=torch.float32)[None, ...]  # (1,H,W,D)\n",
    "        task = torch.tensor(task, dtype=torch.float32)[None, ...]\n",
    "\n",
    "        return rest, task\n",
    "\n",
    "\n",
    "# ===========================================\n",
    "# 2. Stable Encoder & Decoder (no flatten!)\n",
    "# ===========================================\n",
    "\n",
    "# class Encoder3D(nn.Module):\n",
    "#     def __init__(self, latent_dim=128):\n",
    "#         super().__init__()\n",
    "#         self.conv = nn.Sequential(\n",
    "#             nn.Conv3d(1, 32, 3, stride=2, padding=1), nn.LeakyReLU(0.01, inplace=True),\n",
    "#             nn.Conv3d(32, 64, 3, stride=2, padding=1), nn.LeakyReLU(0.01, inplace=True),\n",
    "#             nn.Conv3d(64, 128, 3, stride=2, padding=1), nn.LeakyReLU(0.01, inplace=True),\n",
    "#             nn.Conv3d(128, 256, 3, stride=2, padding=1), nn.LeakyReLU(0.01, inplace=True),\n",
    "#         )\n",
    "#         self.to_latent = nn.Conv3d(256, latent_dim, kernel_size=1)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         h = self.conv(x)\n",
    "#         h_lat = self.to_latent(h)\n",
    "#         #z = torch.mean(h_lat, dim=[2,3,4])\n",
    "#         h = self.conv(x)                 # (B,C,H',W',D')\n",
    "#         h_flat = h.view(B, -1)\n",
    "#         z = self.fc_mu(h_flat)           # (B, latent_dim)\n",
    "\n",
    "#         return z, h_lat.shape\n",
    "\n",
    "# class Decoder3D(nn.Module):\n",
    "#     def __init__(self, latent_dim=128):\n",
    "#         super().__init__()\n",
    "#         self.expand = nn.ConvTranspose3d(latent_dim, 256, 4, stride=1)\n",
    "#         self.deconv = nn.Sequential(\n",
    "#             nn.ConvTranspose3d(256, 128, 4, stride=2, padding=1), nn.LeakyReLU(0.01, inplace=True),\n",
    "#             nn.ConvTranspose3d(128, 64, 4, stride=2, padding=1),  nn.LeakyReLU(0.01, inplace=True),\n",
    "#             nn.ConvTranspose3d(64, 32, 4, stride=2, padding=1),   nn.LeakyReLU(0.01, inplace=True),\n",
    "#             nn.ConvTranspose3d(32, 1, 4, stride=2, padding=1),\n",
    "#         )\n",
    "#     def forward(self, z, orig_shape):\n",
    "#         B, _, H0, W0, D0 = orig_shape\n",
    "#         h = z[:, :, None, None, None]\n",
    "#         h = self.expand(h)\n",
    "#         x_hat = self.deconv(h)\n",
    "#         x_hat = F.interpolate(x_hat, size=(H0, W0, D0),\n",
    "#                               mode=\"trilinear\", align_corners=False)\n",
    "#         return x_hat\n",
    "\n",
    "\n",
    "# # ===========================================\n",
    "# # 3. Load subject ID list\n",
    "# # ===========================================\n",
    "\n",
    "# def load_ids(path):\n",
    "#     with open(path, \"r\") as f:\n",
    "#         return [line.strip() for line in f.readlines()]\n",
    "\n",
    "\n",
    "# ===========================================\n",
    "# 4. Masked MSE Loss\n",
    "# ===========================================\n",
    "\n",
    "# def masked_mse(X_hat, X, mask_t):\n",
    "#     \"\"\"\n",
    "#     Compute MSE only inside mask.\n",
    "#     mask_t: (1,1,H,W,D) tensor\n",
    "#     \"\"\"\n",
    "#     err = (X_hat - X) ** 2\n",
    "#     masked_err = err * mask_t\n",
    "#     return masked_err.sum() / (mask_t.sum() + 1e-6)\n",
    "\n",
    "def masked_mse(X_hat, X, mask_t):\n",
    "    \"\"\"\n",
    "    Batched masked MSE:\n",
    "        Returns mean over batch of (MSE on masked region).\n",
    "    X_hat: (B,1,H,W,D)\n",
    "    X:     (B,1,H,W,D)\n",
    "    mask_t: (1,1,H,W,D)\n",
    "    \"\"\"\n",
    "    # Broadcast mask to batch\n",
    "    mask = mask_t   # shape (1,1,H,W,D), auto-broadcasts\n",
    "\n",
    "    # compute squared error inside mask\n",
    "    err = (X_hat - X) ** 2              # (B,1,H,W,D)\n",
    "    masked_err = err * mask             # (B,1,H,W,D)\n",
    "\n",
    "    # sum error over spatial dims for each batch element\n",
    "    spatial_error = masked_err.view(X.shape[0], -1).sum(dim=1)  # (B,)\n",
    "\n",
    "    # number of valid voxels (same for each batch element)\n",
    "    denom = mask.sum() + 1e-6\n",
    "\n",
    "    # MSE per subject\n",
    "    mse_per_subject = spatial_error / denom   # (B,)\n",
    "\n",
    "    # average over batch\n",
    "    return mse_per_subject.mean()\n",
    "\n",
    "\n",
    "\n",
    "def compute_global_stats(rest_dir, task_dir, train_ids, mask_np):\n",
    "    vals = []\n",
    "    for sid in train_ids:\n",
    "        rest = nib.load(os.path.join(rest_dir, f\"{sid}_fALFF.nii\")).get_fdata()\n",
    "        task = nib.load(os.path.join(task_dir, f\"{sid}_2bk-0bk.nii\")).get_fdata()\n",
    "        rest = np.nan_to_num(rest)\n",
    "        task = np.nan_to_num(task)\n",
    "\n",
    "        if mask_np is not None:\n",
    "            rest = rest * mask_np\n",
    "            task = task * mask_np\n",
    "\n",
    "        vals.append(rest.flatten())\n",
    "        vals.append(task.flatten())\n",
    "\n",
    "    vals = np.concatenate(vals)\n",
    "    mu = vals.mean()\n",
    "    std = vals.std()\n",
    "    np.save(\"global_mean.npy\", mu)\n",
    "    np.save(\"global_std.npy\", std)\n",
    "    return mu, std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "49f962d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class ResidualBlock3D(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv3d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.norm1 = nn.InstanceNorm3d(channels)\n",
    "        self.conv2 = nn.Conv3d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.norm2 = nn.InstanceNorm3d(channels)\n",
    "        self.act = nn.LeakyReLU(0.01, inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.norm1(out)\n",
    "        out = self.act(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.norm2(out)\n",
    "\n",
    "        out = out + x\n",
    "        out = self.act(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DownBlock3D(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.down = nn.Conv3d(in_ch, out_ch, kernel_size=3, stride=2, padding=1)\n",
    "        self.norm = nn.InstanceNorm3d(out_ch)\n",
    "        self.act = nn.LeakyReLU(0.01, inplace=True)\n",
    "        self.res = ResidualBlock3D(out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.down(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.act(x)\n",
    "        x = self.res(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UpBlock3D(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose3d(in_ch, out_ch, kernel_size=4, stride=2, padding=1)\n",
    "        self.norm = nn.InstanceNorm3d(out_ch)\n",
    "        self.act = nn.LeakyReLU(0.01, inplace=True)\n",
    "        self.res = ResidualBlock3D(out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.act(x)\n",
    "        x = self.res(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cb144c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder3DStrong(nn.Module):\n",
    "    def __init__(self, latent_dim=128, base_channels=16):\n",
    "        super().__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.base = base_channels\n",
    "\n",
    "        # Initial conv + residual\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv3d(1, base_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.InstanceNorm3d(base_channels),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "            ResidualBlock3D(base_channels),\n",
    "        )\n",
    "\n",
    "        # Downsampling path\n",
    "        self.down1 = DownBlock3D(base_channels, base_channels * 2)   # 16 -> 32\n",
    "        self.down2 = DownBlock3D(base_channels * 2, base_channels * 4)  # 32 -> 64\n",
    "        self.down3 = DownBlock3D(base_channels * 4, base_channels * 8)  # 64 -> 128\n",
    "\n",
    "        # Latent projection\n",
    "        self.fc_mu = None  # initialized lazily after seeing first input\n",
    "\n",
    "    def _build_fc(self, feature_shape):\n",
    "        \"\"\"\n",
    "        Called once after we know the conv output spatial size.\n",
    "        feature_shape = (C, H, W, D)\n",
    "        \"\"\"\n",
    "        C, H, W, D = feature_shape\n",
    "        self.flat_dim = C * H * W * D\n",
    "        self.fc_mu = nn.Linear(self.flat_dim, self.latent_dim)\n",
    "        # optional: weight init\n",
    "        nn.init.xavier_uniform_(self.fc_mu.weight)\n",
    "        self.fc_mu.to(next(self.parameters()).device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (B,1,H,W,D)\n",
    "        returns:\n",
    "          z: (B, latent_dim)\n",
    "          feature_shape: (C, H', W', D') needed by decoder\n",
    "        \"\"\"\n",
    "        h = self.stem(x)          # (B, C, H, W, D)\n",
    "        h = self.down1(h)         # (B, 2C, H/2, W/2, D/2)\n",
    "        h = self.down2(h)         # (B, 4C, ...)\n",
    "        h = self.down3(h)         # (B, 8C, ...)\n",
    "\n",
    "        B, C, H, W, D = h.shape\n",
    "        feature_shape = (C, H, W, D)\n",
    "\n",
    "        # lazy init fc layer (so it adapts to actual spatial size)\n",
    "        if self.fc_mu is None:\n",
    "            self._build_fc(feature_shape)\n",
    "\n",
    "        h_flat = h.view(B, -1)\n",
    "        z = self.fc_mu(h_flat)\n",
    "        return z, feature_shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea2f36fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder3DStrong(nn.Module):\n",
    "    def __init__(self, latent_dim, feature_shape, out_spatial_shape):\n",
    "        \"\"\"\n",
    "        feature_shape: (C, Hf, Wf, Df) from encoder\n",
    "        out_spatial_shape: (H0, W0, D0) = original fMRI volume size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        C, Hf, Wf, Df = feature_shape\n",
    "        self.feature_shape = (C, Hf, Wf, Df)\n",
    "        self.flat_dim = C * Hf * Wf * Df\n",
    "\n",
    "        H0, W0, D0 = out_spatial_shape\n",
    "        self.out_spatial_shape = (H0, W0, D0)\n",
    "\n",
    "        # latent → feature\n",
    "        self.fc = nn.Linear(latent_dim, self.flat_dim)\n",
    "\n",
    "        self.up = nn.Sequential(\n",
    "            nn.ConvTranspose3d(C, 128, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "\n",
    "            nn.ConvTranspose3d(128, 64, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "\n",
    "            nn.ConvTranspose3d(64, 32, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "\n",
    "            nn.Conv3d(32, 1, 3, padding=1),\n",
    "        )\n",
    "\n",
    "    def forward(self, z, target_shape=None):\n",
    "        \"\"\"\n",
    "        z: (B, latent_dim)\n",
    "        target_shape: optional (B,1,H,W,D). If None, we use self.out_spatial_shape.\n",
    "        \"\"\"\n",
    "        B = z.size(0)\n",
    "\n",
    "        h = self.fc(z)\n",
    "        h = h.view(B, *self.feature_shape)   # (B, C, Hf, Wf, Df)\n",
    "        x_hat = self.up(h)                   # (B, 1, H*, W*, D*)\n",
    "\n",
    "        # --- always interpolate to desired output size ---\n",
    "        if target_shape is not None:\n",
    "            _, _, H, W, D = target_shape\n",
    "        else:\n",
    "            H, W, D = self.out_spatial_shape\n",
    "\n",
    "        x_hat = F.interpolate(\n",
    "            x_hat,\n",
    "            size=(H, W, D),\n",
    "            mode=\"trilinear\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "\n",
    "        return x_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc317dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# ----------------------------------------\n",
    "# Basic U-Net building blocks (3D)\n",
    "# ----------------------------------------\n",
    "\n",
    "class DoubleConv3D(nn.Module):\n",
    "    \"\"\"\n",
    "    (Conv3d -> InstanceNorm3d -> LeakyReLU) * 2\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv3d(in_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.InstanceNorm3d(out_ch),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "\n",
    "            nn.Conv3d(out_ch, out_ch, kernel_size=3, padding=1),\n",
    "            nn.InstanceNorm3d(out_ch),\n",
    "            nn.LeakyReLU(0.01, inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Down3D(nn.Module):\n",
    "    \"\"\"\n",
    "    Downsampling: MaxPool3d(2) + DoubleConv3D\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.pool = nn.MaxPool3d(kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConv3D(in_ch, out_ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def center_crop_3d(x, target_dhw):\n",
    "    \"\"\"\n",
    "    Center-crop a 5D tensor (B,C,D,H,W) to target spatial size (D_t,H_t,W_t).\n",
    "    This handles small off-by-one mismatches from upsampling / pooling.\n",
    "    \"\"\"\n",
    "    _, _, D, H, W = x.shape\n",
    "    D_t, H_t, W_t = target_dhw\n",
    "\n",
    "    d_start = max((D - D_t) // 2, 0)\n",
    "    h_start = max((H - H_t) // 2, 0)\n",
    "    w_start = max((W - W_t) // 2, 0)\n",
    "\n",
    "    d_end = d_start + D_t\n",
    "    h_end = h_start + H_t\n",
    "    w_end = w_start + W_t\n",
    "\n",
    "    return x[:, :, d_start:d_end, h_start:h_end, w_start:w_end]\n",
    "\n",
    "\n",
    "class Up3D(nn.Module):\n",
    "    \"\"\"\n",
    "    Upsampling block: ConvTranspose3d + concat skip + DoubleConv3D\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        \"\"\"\n",
    "        in_ch: channels of feature BEFORE concat (from decoder)\n",
    "        out_ch: output channels AFTER DoubleConv\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Reduce spatial size mismatch via transposed conv\n",
    "        self.up = nn.ConvTranspose3d(\n",
    "            in_ch, in_ch // 2,\n",
    "            kernel_size=2, stride=2\n",
    "        )\n",
    "        self.conv = DoubleConv3D(in_ch, out_ch)  # (decoder_ch + skip_ch)\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        # x: decoder feature, skip: encoder feature\n",
    "        x = self.up(x)        # (B, in_ch//2, D*,H*,W*)\n",
    "\n",
    "        # crop skip to match x spatial size\n",
    "        _, _, D, H, W = x.shape\n",
    "        skip_c = center_crop_3d(skip, (D, H, W))\n",
    "\n",
    "        # concat along channels\n",
    "        x = torch.cat([x, skip_c], dim=1)  # (B, in_ch, D,H,W)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class OutConv3D(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch=1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv3d(in_ch, out_ch, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f60f09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet3DAE(nn.Module):\n",
    "    \"\"\"\n",
    "    3D U-Net style autoencoder for fMRI volumes.\n",
    "    Returns:\n",
    "      x_hat: reconstruction (B,1,D,H,W)\n",
    "      z: latent vector (B, latent_dim) from bottleneck features\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels=1, base_ch=16, latent_dim=128):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.base_ch = base_ch\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # ------------- Encoder (U-Net down path) -------------\n",
    "        self.inc   = DoubleConv3D(in_channels, base_ch)         # level 0\n",
    "        self.down1 = Down3D(base_ch,       base_ch * 2)         # level 1\n",
    "        self.down2 = Down3D(base_ch * 2,   base_ch * 4)         # level 2\n",
    "        self.down3 = Down3D(base_ch * 4,   base_ch * 8)         # level 3\n",
    "        # one more down for bottleneck\n",
    "        self.down4 = Down3D(base_ch * 8,   base_ch * 16)        # bottleneck\n",
    "\n",
    "        # ------------- Latent head (for SB, etc.) -------------\n",
    "        self.global_pool = nn.AdaptiveAvgPool3d(1)  # (B,C,1,1,1)\n",
    "        self.fc_mu = nn.Linear(base_ch * 16, latent_dim)\n",
    "\n",
    "        # ------------- Decoder (U-Net up path) -------------\n",
    "        self.up1 = Up3D(base_ch * 16, base_ch * 8)  # bottleneck + skip3\n",
    "        self.up2 = Up3D(base_ch * 8,  base_ch * 4)  # dec + skip2\n",
    "        self.up3 = Up3D(base_ch * 4,  base_ch * 2)  # dec + skip1\n",
    "        self.up4 = Up3D(base_ch * 2,  base_ch)      # dec + skip0\n",
    "\n",
    "        self.outc = OutConv3D(base_ch, 1)\n",
    "\n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        Encoder only: returns latent z and bottleneck feature for debugging.\n",
    "        \"\"\"\n",
    "        x0 = self.inc(x)       # (B, base,   D0,H0,W0)\n",
    "        x1 = self.down1(x0)    # (B, 2base,  D1,H1,W1)\n",
    "        x2 = self.down2(x1)    # (B, 4base,  D2,H2,W2)\n",
    "        x3 = self.down3(x2)    # (B, 8base,  D3,H3,W3)\n",
    "        x4 = self.down4(x3)    # (B,16base,  D4,H4,W4) bottleneck\n",
    "\n",
    "        # latent vector from bottleneck\n",
    "        pooled = self.global_pool(x4).view(x4.size(0), -1)  # (B, 16*base)\n",
    "        z = self.fc_mu(pooled)                              # (B, latent_dim)\n",
    "        return z, (x0, x1, x2, x3, x4)\n",
    "\n",
    "    def decode(self, x0, x1, x2, x3, x4, out_shape):\n",
    "        \"\"\"\n",
    "        Decoder given encoder feature maps.\n",
    "        out_shape: (B,1,D0,H0,W0) of original input, used to final-resize.\n",
    "        \"\"\"\n",
    "        x = self.up1(x4, x3)      # (B,8base, D3',H3',W3')\n",
    "        x = self.up2(x,  x2)      # (B,4base, D2',H2',W2')\n",
    "        x = self.up3(x,  x1)      # (B,2base, D1',H1',W1')\n",
    "        x = self.up4(x,  x0)      # (B,base,  D0',H0',W0')\n",
    "\n",
    "        x = self.outc(x)          # (B,1,D*,H*,W*)\n",
    "\n",
    "        # Final interpolation to EXACT input size (handles odd dims like 91x109x91)\n",
    "        _, _, D0, H0, W0 = out_shape\n",
    "        if (x.shape[2] != D0) or (x.shape[3] != H0) or (x.shape[4] != W0):\n",
    "            x = F.interpolate(\n",
    "                x,\n",
    "                size=(D0, H0, W0),\n",
    "                mode=\"trilinear\",\n",
    "                align_corners=False,\n",
    "            )\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Full AE: input -> reconstruction + latent.\n",
    "        x: (B,1,D,H,W)\n",
    "        returns:\n",
    "          x_hat: (B,1,D,H,W)\n",
    "          z:     (B, latent_dim)\n",
    "        \"\"\"\n",
    "        B, C, D0, H0, W0 = x.shape\n",
    "\n",
    "        z, (x0, x1, x2, x3, x4) = self.encode(x)\n",
    "        x_hat = self.decode(x0, x1, x2, x3, x4, x.shape)\n",
    "        return x_hat, z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12cc702",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c095b47c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 152\u001b[39m\n\u001b[32m    148\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining complete. Best val:\u001b[39m\u001b[33m\"\u001b[39m, best_val)\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     19\u001b[39m     device = \u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     21\u001b[39m     \u001b[38;5;66;03m# -------- Dataset & Dataloaders --------\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     mu, std = \u001b[43mcompute_global_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrest_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_np\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m     train_ds = HCPRestTaskDataset(rest_dir, task_dir, train_ids, mask_np=mask_np,\n\u001b[32m     24\u001b[39m                               normalize=\u001b[33m\"\u001b[39m\u001b[33mzscore\u001b[39m\u001b[33m\"\u001b[39m, global_mean=mu, global_std=std)\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m#     train_ds = HCPRestTaskDataset(\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m#         rest_dir, task_dir, train_ids, mask_np=mask_np\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m#     )\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 174\u001b[39m, in \u001b[36mcompute_global_stats\u001b[39m\u001b[34m(rest_dir, task_dir, train_ids, mask_np)\u001b[39m\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m sid \u001b[38;5;129;01min\u001b[39;00m train_ids:\n\u001b[32m    173\u001b[39m     rest = nib.load(os.path.join(rest_dir, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msid\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_fALFF.nii\u001b[39m\u001b[33m\"\u001b[39m)).get_fdata()\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m     task = \u001b[43mnib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43msid\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m_2bk-0bk.nii\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.get_fdata()\n\u001b[32m    175\u001b[39m     rest = np.nan_to_num(rest)\n\u001b[32m    176\u001b[39m     task = np.nan_to_num(task)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/nibabel/loadsave.py:111\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(filename, **kwargs)\u001b[39m\n\u001b[32m    109\u001b[39m     is_valid, sniff = image_klass.path_maybe_image(filename, sniff)\n\u001b[32m    110\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_valid:\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m         img = \u001b[43mimage_klass\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_filename\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m img\n\u001b[32m    114\u001b[39m matches, msg = _signature_matches_extension(filename)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/nibabel/dataobj_images.py:504\u001b[39m, in \u001b[36mDataobjImage.from_filename\u001b[39m\u001b[34m(klass, filename, mmap, keep_file_open)\u001b[39m\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mmmap should be one of \u001b[39m\u001b[33m{\u001b[39m\u001b[33mTrue, False, \u001b[39m\u001b[33m'\u001b[39m\u001b[33mc\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m}\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    503\u001b[39m file_map = klass.filespec_to_file_map(filename)\n\u001b[32m--> \u001b[39m\u001b[32m504\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mklass\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_file_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmmap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmmap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep_file_open\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeep_file_open\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/nibabel/analyze.py:962\u001b[39m, in \u001b[36mAnalyzeImage.from_file_map\u001b[39m\u001b[34m(klass, file_map, mmap, keep_file_open)\u001b[39m\n\u001b[32m    960\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mmmap should be one of \u001b[39m\u001b[33m{\u001b[39m\u001b[33mTrue, False, \u001b[39m\u001b[33m'\u001b[39m\u001b[33mc\u001b[39m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33m}\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    961\u001b[39m hdr_fh, img_fh = klass._get_fileholders(file_map)\n\u001b[32m--> \u001b[39m\u001b[32m962\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mhdr_fh\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_prepare_fileobj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m hdrf:\n\u001b[32m    963\u001b[39m     header = klass.header_class.from_fileobj(hdrf)\n\u001b[32m    964\u001b[39m hdr_copy = header.copy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/nibabel/fileholders.py:77\u001b[39m, in \u001b[36mFileHolder.get_prepare_fileobj\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     75\u001b[39m     obj.seek(\u001b[38;5;28mself\u001b[39m.pos)\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m     obj = \u001b[43mImageOpener\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pos != \u001b[32m0\u001b[39m:\n\u001b[32m     79\u001b[39m         obj.seek(\u001b[38;5;28mself\u001b[39m.pos)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/nibabel/openers.py:181\u001b[39m, in \u001b[36mOpener.__init__\u001b[39m\u001b[34m(self, fileish, *args, **kwargs)\u001b[39m\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# Clear keep_open hint if it is not relevant for the file type\u001b[39;00m\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    180\u001b[39m     kwargs.pop(\u001b[33m'\u001b[39m\u001b[33mkeep_open\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m \u001b[38;5;28mself\u001b[39m.fobj = \u001b[43mopener\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileish\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[38;5;28mself\u001b[39m._name = fileish\n\u001b[32m    183\u001b[39m \u001b[38;5;28mself\u001b[39m.me_opened = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# 5. Main AE Training Function\n",
    "# ===========================================\n",
    "\n",
    "def main():\n",
    "    # -------- Paths --------\n",
    "    rest_dir = \"/nfs/turbo/sph-jianghui1/yutingd/HCP/fALFF/\"\n",
    "    task_dir = \"/nfs/turbo/sph-jianghui1/yutingd/HCP/WM_Contrasts_2bk-0bk/\"\n",
    "    mask_path = \"/nfs/turbo/sph-jianghui1/yutingd/HCP/AAL_MNI_2mm.nii\"\n",
    "\n",
    "    train_ids = load_ids(\"/nfs/turbo/sph-jianghui1/yutingd/HCP/training_ids.txt\")\n",
    "    val_ids   = load_ids(\"/nfs/turbo/sph-jianghui1/yutingd/HCP/validation_ids.txt\")\n",
    "\n",
    "    # -------- Load Mask --------\n",
    "    mask_np = nib.load(mask_path).get_fdata()\n",
    "    mask_np = (mask_np > 0).astype(np.float32)\n",
    "\n",
    "    # -------- Device --------\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # -------- Dataset & Dataloaders --------\n",
    "    mu, std = compute_global_stats(rest_dir, task_dir, train_ids, mask_np)\n",
    "    train_ds = HCPRestTaskDataset(rest_dir, task_dir, train_ids, mask_np=mask_np,\n",
    "                              normalize=\"zscore\", global_mean=mu, global_std=std)\n",
    "\n",
    "#     train_ds = HCPRestTaskDataset(\n",
    "#         rest_dir, task_dir, train_ids, mask_np=mask_np\n",
    "#     )\n",
    "    val_ds = HCPRestTaskDataset(\n",
    "        rest_dir, task_dir, val_ids, mask_np=mask_np,normalize=\"zscore\", global_mean=mu, global_std=std\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=4, shuffle=True)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=4, shuffle=False)\n",
    "\n",
    "    # -------- Mask tensor for loss --------\n",
    "    mask_t = torch.tensor(mask_np, dtype=torch.float32, device=device)[None, None]\n",
    "\n",
    "    # -------- AE Models --------\n",
    "    latent_dim = 128\n",
    "    encoder = Encoder3DStrong(latent_dim).to(device)\n",
    "\n",
    "    # run 1 sample through encoder to get feature_shape\n",
    "    rest, task = next(iter(train_loader))\n",
    "    #X = torch.cat([rest, task], dim=0).to(device)\n",
    "    X = task.to(device)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        z, feature_shape = encoder(X)\n",
    "    _, _, H0, W0, D0 = X.shape\n",
    "\n",
    "    decoder = Decoder3DStrong(\n",
    "        latent_dim,\n",
    "        feature_shape=feature_shape,\n",
    "        out_spatial_shape=(H0, W0, D0),\n",
    "    ).to(device)\n",
    "    #decoder = Decoder3DStrong(latent_dim, feature_shape).to(device)\n",
    "    #encoder = Encoder3D(latent_dim).to(device)\n",
    "    #decoder = Decoder3D(latent_dim).to(device)\n",
    "#     encoder = Encoder3DStrong(latent_dim=128, base_channels=16).to(device)\n",
    "#     decoder = Decoder3DStrong(latent_dim=128, base_channels=16).to(device)\n",
    "    params = list(encoder.parameters()) + list(decoder.parameters())\n",
    "    optimizer = torch.optim.AdamW(params, lr=5e-4, weight_decay=1e-5)\n",
    "\n",
    "\n",
    "    #params = list(encoder.parameters()) + list(decoder.parameters())\n",
    "    #optimizer = torch.optim.Adam(params, lr=1e-3)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "#     latent_dim = 128\n",
    "#     ae = UNet3DAE(in_channels=1, base_ch=16, latent_dim=latent_dim).to(device)\n",
    "\n",
    "#     params = list(ae.parameters())\n",
    "#     optimizer = torch.optim.AdamW(params, lr=5e-4, weight_decay=1e-5)\n",
    "#     scaler = GradScaler()\n",
    "\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    best_train = float(\"inf\")\n",
    "    epochs = 50\n",
    "\n",
    "    # -------- Training Loop --------\n",
    "    for epoch in range(1, epochs+1):\n",
    "        encoder.train()\n",
    "        decoder.train()\n",
    "        train_loss = 0\n",
    "\n",
    "        for rest, task in tqdm(train_loader, desc=f\"Epoch {epoch} [train]\"):\n",
    "            #X = torch.cat([rest, task], dim=0).to(device)  # (2,1,H,W,D)\n",
    "            X = task.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast():\n",
    "                z, feat_shape = encoder(X)\n",
    "                #X_hat = decoder(z, X.shape)\n",
    "                X_hat = decoder(z) \n",
    "\n",
    "                loss = masked_mse(X_hat, X, mask_t)\n",
    "                  \n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(params, 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "        # -------- Save best --------\n",
    "        if train_loss < best_train:\n",
    "            best_train = train_loss\n",
    "            torch.save(encoder.state_dict(), \"encoder_ae_train.pth\")\n",
    "            torch.save(decoder.state_dict(), \"decoder_ae_train.pth\")\n",
    "            print(\"  → Saved new best AE train model\")\n",
    "\n",
    "\n",
    "        # -------- Validation --------\n",
    "        encoder.eval()\n",
    "        decoder.eval()\n",
    "        val_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for rest, task in tqdm(val_loader, desc=f\"Epoch {epoch} [val]\"):\n",
    "                #X = torch.cat([rest, task], dim=0).to(device)\n",
    "                X = task.to(device)\n",
    "\n",
    "                with autocast():\n",
    "                    z, feat_shape = encoder(X)\n",
    "                    X_hat = decoder(z)\n",
    "                    #X_hat, z = ae(X)\n",
    "                    loss = masked_mse(X_hat, X, mask_t)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch} | train={train_loss:.4f} | val={val_loss:.4f}\")\n",
    "        \n",
    "        # -------- Save best --------\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            torch.save(encoder.state_dict(), \"encoder_ae_best.pth\")\n",
    "            torch.save(decoder.state_dict(), \"decoder_ae_best.pth\")\n",
    "            print(\"  → Saved new best AE model\")\n",
    "\n",
    "    print(\"Training complete. Best val:\", best_val)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c1bbc539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 91, 109, 91])\n",
      "torch.Size([1, 1, 91, 109, 91])\n",
      "MSE: 3.5825729370117188\n",
      "corr: 0.5495498180389404\n",
      "ssim: 0.11813586950302124\n",
      "Task true  mean/std: 4.3276364714017745e-09 1.0000005960464478\n",
      "Task recon mean/std: -0.35254940390586853 0.8404914736747742\n",
      "Ratio (recon/true): -81464653.10467112\n"
     ]
    }
   ],
   "source": [
    "rest, task = next(iter(test_loader))\n",
    "task = task.to(device)\n",
    "#rest = rest.to(device)\n",
    "mask_np = nib.load(mask_path).get_fdata()\n",
    "mask_np = (mask_np > 0).astype(np.float32)\n",
    "mask_t  = torch.tensor(mask_np, dtype=torch.float32)[None, None]\n",
    "mask_t = mask_t.to(device)\n",
    "\n",
    "print(mask_t.shape)\n",
    "print(task.shape)\n",
    "\n",
    "if mask_t.shape[0] != task.shape[0]:\n",
    "    mask_batched = mask_t.repeat(task.shape[0], 1, 1, 1, 1)\n",
    "else:\n",
    "    mask_batched = mask_t\n",
    "with torch.no_grad():\n",
    "    z, _ = encoder(task)\n",
    "    task_recon = decoder(z, task.shape)\n",
    "    \n",
    "    #zr, _ = encoder(rest)\n",
    "    #rest_recon = decoder(zr, rest.shape)\n",
    "    \n",
    "mse = masked_mse(task_recon, task, mask_batched).item()\n",
    "corr = masked_corr(task_recon, task, mask_batched).item()\n",
    "ssim = masked_ssim_3d(task_recon, task, mask_batched).item()\n",
    "print(\"MSE:\", mse)\n",
    "print(\"corr:\", corr)\n",
    "print(\"ssim:\", ssim)\n",
    "\n",
    "\n",
    "print(\"Task true  mean/std:\", task.mean().item(), task.std().item())\n",
    "print(\"Task recon mean/std:\", task_recon.mean().item(), task_recon.std().item())\n",
    "\n",
    "    \n",
    "#print(\"Rest true  mean/std:\", rest.mean().item(), rest.std().item())\n",
    "#print(\"Rest recon mean/std:\", rest_recon.mean().item(), rest_recon.std().item())\n",
    "\n",
    "print(\"Ratio (recon/true):\", \n",
    "      task_recon.mean().item()/task.mean().item() if task.mean().item()!=0 else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7072bf86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] Loaded 499 subjects\n",
      "[Dataset] Loaded 142 subjects\n",
      "[Dataset] Loaded 73 subjects\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1754538/3782407550.py:45: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  encoder.load_state_dict(torch.load(\"encoder_ae_best.pth\", map_location=device))\n",
      "/tmp/ipykernel_1754538/3782407550.py:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  decoder.load_state_dict(torch.load(\"decoder_ae_best.pth\", map_location=device))\n"
     ]
    }
   ],
   "source": [
    "rest_dir = \"/nfs/turbo/sph-jianghui1/yutingd/HCP/fALFF/\"\n",
    "task_dir = \"/nfs/turbo/sph-jianghui1/yutingd/HCP/WM_Contrasts_2bk-0bk/\"\n",
    "mask_path = \"/nfs/turbo/sph-jianghui1/yutingd/HCP/AAL_MNI_2mm.nii\"\n",
    "\n",
    "train_ids = load_ids(\"/nfs/turbo/sph-jianghui1/yutingd/HCP/training_ids.txt\")\n",
    "val_ids   = load_ids(\"/nfs/turbo/sph-jianghui1/yutingd/HCP/validation_ids.txt\")\n",
    "\n",
    "test_ids  = load_ids(\"/nfs/turbo/sph-jianghui1/yutingd/HCP/test_ids.txt\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# -------- Load mask --------\n",
    "mask_np = nib.load(mask_path).get_fdata()\n",
    "mask_np = (mask_np > 0).astype(np.float32)\n",
    "mask_t = torch.tensor(mask_np, dtype=torch.float32, device=device)[None, None]\n",
    "\n",
    "# -------- Datasets & loaders --------\n",
    "train_ds = HCPRestTaskDataset(rest_dir, task_dir, train_ids, mask_np=mask_np)\n",
    "val_ds   = HCPRestTaskDataset(rest_dir, task_dir, val_ids, mask_np=mask_np)\n",
    "test_ds  = HCPRestTaskDataset(rest_dir, task_dir, test_ids, mask_np=mask_np)\n",
    "\n",
    "B = 16\n",
    "train_loader = DataLoader(train_ds, batch_size=B, shuffle=True, drop_last=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=B, shuffle=False, drop_last=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=1, shuffle=False)\n",
    "\n",
    "# -------- Load trained AE --------\n",
    "encoder = Encoder3DStrong(latent_dim).to(device)\n",
    "\n",
    "# run 1 sample through encoder to get feature_shape\n",
    "rest, task = next(iter(train_loader))\n",
    "#X = torch.cat([rest, task], dim=0).to(device)\n",
    "X = task.to(device)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    z, feature_shape = encoder(X)\n",
    "_, _, H0, W0, D0 = X.shape\n",
    "\n",
    "decoder = Decoder3DStrong(\n",
    "    latent_dim,\n",
    "    feature_shape=feature_shape,\n",
    "    out_spatial_shape=(H0, W0, D0),\n",
    ").to(device)\n",
    "encoder.load_state_dict(torch.load(\"encoder_ae_best.pth\", map_location=device))\n",
    "decoder.load_state_dict(torch.load(\"decoder_ae_best.pth\", map_location=device))\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "for p in encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in decoder.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "# epoch = 1\n",
    "# for rest, task in tqdm(train_loader, desc=f\"SB Epoch {epoch} [train]\"):\n",
    "#     rest = rest.to(device)   # (1,1,H,W,D)\n",
    "#     task = task.to(device)\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         z, _ = encoder(rest)  # (1,d)\n",
    "#         rest_recon = decoder(z, task.shape)\n",
    "        \n",
    "        \n",
    "#         z_r,_ = encoder(rest)\n",
    "#         z_t,_ = encoder(task)\n",
    "\n",
    "#         print(\"Rest latent norm:\", z_r.norm(dim=1).mean().item())\n",
    "#         print(\"Task latent norm:\", z_t.norm(dim=1).mean().item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "842daf02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task true  mean/std: -1.0142897632903214e-09 1.0\n",
      "Rest recon mean/std: 0.5522738099098206 1.1539183855056763\n",
      "Ratio (recon/true): -82917735.36789353\n"
     ]
    }
   ],
   "source": [
    "print(\"Task true  mean/std:\", rest.mean().item(), rest.std().item())\n",
    "print(\"Rest recon mean/std:\", rest_recon.mean().item(), rest_recon.std().item())\n",
    "print(\"Ratio (recon/true):\", \n",
    "      rest_recon.mean().item()/task.mean().item() if task.mean().item()!=0 else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b88ab241",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.eval()\n",
    "all_z = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for rest, task in train_loader:\n",
    "        rest = rest.to(device)\n",
    "        task = task.to(device)\n",
    "\n",
    "        z_r, _ = encoder(rest)\n",
    "        z_t, _ = encoder(task)\n",
    "\n",
    "        all_z.append(z_r.cpu())\n",
    "        all_z.append(z_t.cpu())\n",
    "\n",
    "all_z = torch.cat(all_z, dim=0)        # (N_total, latent_dim)\n",
    "z_mean = all_z.mean(dim=0)            # (latent_dim,)\n",
    "z_std  = all_z.std(dim=0) + 1e-6      # avoid zeros\n",
    "\n",
    "torch.save({\"mean\": z_mean, \"std\": z_std}, \"latent_stats.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "334a6952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.5712,  3.1305, -4.8395,  4.4951,  0.9114, -5.6921,  0.2736,  2.9826,\n",
      "        -0.0211,  0.3143, -3.0756, -4.0493,  0.5063, -3.2029,  5.6924, -2.8474,\n",
      "         2.9180,  1.9802,  5.1454, -3.6219,  3.9465, -2.0529, -0.5626, -3.6194,\n",
      "        -1.1389, -4.2627, -2.6841, -5.2748, -6.5108, -0.7909,  0.0553,  3.6960,\n",
      "        -3.7562,  4.7947,  3.6051, -2.8303,  6.8552, -4.8011,  1.0891, -3.3245,\n",
      "         2.5340,  5.5578,  2.9018,  3.5488, -6.2206, -3.6326, -5.7298, -3.6517,\n",
      "         1.8035,  7.1596,  0.4957, -2.8218,  5.7257,  4.6408, -3.5853, -2.0040,\n",
      "         2.8357,  3.3378, -3.9196,  0.3642, -4.4083, -5.4960,  0.2924, -0.8236,\n",
      "         0.1083, -0.1733, -3.1912,  2.5541, -0.2050, -5.7374,  1.4797,  2.9771,\n",
      "        -5.0296,  0.9904,  3.8877, -1.3343,  4.2282,  0.5186,  6.0291, -5.7672,\n",
      "         4.1937,  3.2922,  0.6151,  1.3207, -3.2521,  3.7449,  1.4001, -1.5807,\n",
      "         2.6302,  6.8396,  3.1028, -3.4176, -2.5884,  3.3322, -2.3514,  3.7217,\n",
      "         2.1221,  3.9591,  3.3379,  1.2768,  2.7001, -1.6295, -2.4650,  5.1694,\n",
      "        -4.9202, -5.2783, -2.0725, -4.0656,  5.5398,  2.8440, -1.4015,  1.8714,\n",
      "         3.7047,  0.5127,  5.1914, -0.6929, -8.5665,  3.5901, -3.6219,  0.3042,\n",
      "         1.9683, -1.9252, -5.1470,  4.1041,  3.1853, -1.6240, -1.8163, -1.0341])\n",
      "tensor([2.0573, 1.6761, 0.4594, 1.5175, 1.8646, 1.8983, 1.6055, 0.9721, 1.6566,\n",
      "        1.7946, 0.0973, 1.9351, 2.5008, 0.4874, 2.8254, 0.2177, 0.2544, 1.3568,\n",
      "        0.7089, 0.9761, 0.7184, 2.2975, 2.5909, 0.6322, 2.3964, 1.6802, 2.8180,\n",
      "        1.1758, 2.1488, 2.7800, 1.7738, 1.4675, 0.7515, 0.3052, 2.9935, 3.4567,\n",
      "        2.4607, 3.2302, 2.1724, 1.5995, 2.6283, 2.5570, 2.9078, 0.9976, 3.0107,\n",
      "        0.2469, 1.9182, 2.0101, 1.8627, 1.9175, 1.5786, 1.8376, 1.4042, 3.8825,\n",
      "        1.0060, 1.7843, 2.4954, 1.5468, 1.4123, 1.9993, 0.9832, 0.7588, 2.0552,\n",
      "        2.1420, 0.8915, 1.6111, 3.5545, 3.4785, 1.4304, 0.4067, 3.6657, 1.6297,\n",
      "        2.7626, 2.1206, 0.4266, 3.2684, 0.2517, 2.7271, 0.4415, 1.0734, 2.6669,\n",
      "        0.6003, 2.5865, 0.2781, 1.5959, 3.2503, 0.3632, 2.0407, 0.9997, 2.0307,\n",
      "        2.3717, 0.2925, 3.0340, 2.6020, 0.1604, 2.7786, 2.6769, 1.6481, 2.2621,\n",
      "        2.0692, 0.2493, 1.4637, 2.4568, 1.6595, 0.5085, 1.5581, 1.9163, 0.8116,\n",
      "        1.0028, 3.5515, 2.9234, 2.3395, 2.1136, 1.8367, 2.4590, 1.4506, 3.4245,\n",
      "        1.0744, 3.1104, 1.5385, 1.3861, 2.1498, 0.2859, 1.8033, 0.4964, 2.3741,\n",
      "        1.7304, 1.1382])\n"
     ]
    }
   ],
   "source": [
    "print(z_mean)\n",
    "print(z_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ade3e56",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "980b7dde",
   "metadata": {},
   "source": [
    "# SB Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "37e9675f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_corr(X_hat, X, mask_t, eps=1e-6):\n",
    "    # X_hat, X: (1,1,H,W,D), mask_t: (1,1,H,W,D)\n",
    "    mask_flat = mask_t.view(-1) > 0\n",
    "    x = X_hat.view(-1)[mask_flat]\n",
    "    y = X.view(-1)[mask_flat]\n",
    "\n",
    "    x = x - x.mean()\n",
    "    y = y - y.mean()\n",
    "    num = (x * y).sum()\n",
    "    den = torch.sqrt((x**2).sum() * (y**2).sum() + eps)\n",
    "    return num / den\n",
    "\n",
    "\n",
    "# ===========================================\n",
    "# 4. Drift network & flow-matching loss\n",
    "# ===========================================\n",
    "\n",
    "class DriftNet(nn.Module):\n",
    "    \"\"\"\n",
    "    b_phi(z_t, t, z0): latent drift field for SB / flow-matching.\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim=128, hidden=256):\n",
    "        super().__init__()\n",
    "        self.fc_t = nn.Linear(1, 16)\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim * 2 + 16, hidden),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(hidden, latent_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, zt, t, z0):\n",
    "        \"\"\"\n",
    "        zt, z0: (B,latent_dim)\n",
    "        t: (B,) in [0,1]\n",
    "        \"\"\"\n",
    "        t_emb = self.fc_t(t.unsqueeze(1))  # (B,16)\n",
    "        h = torch.cat([zt, z0, t_emb], dim=1)\n",
    "        return self.net(h)\n",
    "\n",
    "\n",
    "def flow_matching_loss(z0_n, z1_n, drift_net, sigma=0.1):\n",
    "    B, d = z0_n.shape\n",
    "    device = z0_n.device\n",
    "\n",
    "    t = torch.rand(B, device=device)  # (B,)\n",
    "    eps = torch.randn_like(z0_n)\n",
    "\n",
    "    # sample along noisy interpolant\n",
    "    zt_n = (1 - t[:, None]) * z0_n + t[:, None] * z1_n + sigma * eps\n",
    "\n",
    "    v_true = z1_n - z0_n\n",
    "    v_pred = drift_net(zt_n, t, z0_n)\n",
    "\n",
    "    return F.mse_loss(v_pred, v_true)\n",
    "\n",
    "# ===========================================\n",
    "# 5. ODE integration (Euler) in latent space\n",
    "# ===========================================\n",
    "\n",
    "def euler_integrate(z0_n, drift_net, steps=20):\n",
    "    z = z0_n.clone()\n",
    "    dt = 1.0 / steps\n",
    "    for i in range(steps):\n",
    "        t = torch.full((z.size(0),), i * dt, device=z.device)\n",
    "        dz = drift_net(z, t, z0_n)\n",
    "        z = z + dt * dz\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "86cedd67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] Loaded 499 subjects\n",
      "[Dataset] Loaded 142 subjects\n",
      "[Dataset] Loaded 73 subjects\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1754538/2610987264.py:52: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  encoder.load_state_dict(torch.load(\"encoder_ae_best.pth\", map_location=device))\n",
      "/tmp/ipykernel_1754538/2610987264.py:53: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  decoder.load_state_dict(torch.load(\"decoder_ae_best.pth\", map_location=device))\n",
      "/tmp/ipykernel_1754538/2610987264.py:68: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler()\n",
      "/tmp/ipykernel_1754538/2610987264.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  stats = torch.load(\"latent_stats.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SB] Loaded pretrained AE.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SB Epoch 1 [train]:   0%|          | 0/31 [00:00<?, ?it/s]/tmp/ipykernel_1754538/2610987264.py:99: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "SB Epoch 1 [train]: 100%|██████████| 31/31 [00:38<00:00,  1.24s/it]\n",
      "SB Epoch 1 [val]: 100%|██████████| 8/8 [00:11<00:00,  1.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SB] Epoch 1: train=64857.4858, val=59572.6943\n",
      "  → Saved new best SB drift model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SB Epoch 2 [train]: 100%|██████████| 31/31 [00:37<00:00,  1.22s/it]\n",
      "SB Epoch 2 [val]: 100%|██████████| 8/8 [00:11<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SB] Epoch 2: train=51767.8063, val=39505.1111\n",
      "  → Saved new best SB drift model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SB Epoch 3 [train]: 100%|██████████| 31/31 [00:37<00:00,  1.22s/it]\n",
      "SB Epoch 3 [val]: 100%|██████████| 8/8 [00:11<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SB] Epoch 3: train=32086.3844, val=21047.4841\n",
      "  → Saved new best SB drift model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SB Epoch 4 [train]: 100%|██████████| 31/31 [00:37<00:00,  1.22s/it]\n",
      "SB Epoch 4 [val]: 100%|██████████| 8/8 [00:11<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SB] Epoch 4: train=23764.4921, val=18726.9440\n",
      "  → Saved new best SB drift model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SB Epoch 5 [train]: 100%|██████████| 31/31 [00:37<00:00,  1.22s/it]\n",
      "SB Epoch 5 [val]: 100%|██████████| 8/8 [00:11<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SB] Epoch 5: train=22056.2422, val=17401.6134\n",
      "  → Saved new best SB drift model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SB Epoch 6 [train]: 100%|██████████| 31/31 [00:37<00:00,  1.22s/it]\n",
      "SB Epoch 6 [val]: 100%|██████████| 8/8 [00:11<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SB] Epoch 6: train=21017.6712, val=16586.4469\n",
      "  → Saved new best SB drift model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SB Epoch 7 [train]: 100%|██████████| 31/31 [00:37<00:00,  1.22s/it]\n",
      "SB Epoch 7 [val]: 100%|██████████| 8/8 [00:11<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SB] Epoch 7: train=20345.5287, val=15744.1396\n",
      "  → Saved new best SB drift model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SB Epoch 8 [train]: 100%|██████████| 31/31 [00:37<00:00,  1.22s/it]\n",
      "SB Epoch 8 [val]: 100%|██████████| 8/8 [00:11<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SB] Epoch 8: train=19145.2542, val=15305.4436\n",
      "  → Saved new best SB drift model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SB Epoch 9 [train]: 100%|██████████| 31/31 [00:37<00:00,  1.22s/it]\n",
      "SB Epoch 9 [val]: 100%|██████████| 8/8 [00:11<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SB] Epoch 9: train=18623.3470, val=14688.6731\n",
      "  → Saved new best SB drift model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SB Epoch 10 [train]: 100%|██████████| 31/31 [00:37<00:00,  1.22s/it]\n",
      "SB Epoch 10 [val]: 100%|██████████| 8/8 [00:11<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SB] Epoch 10: train=17327.8810, val=13600.5681\n",
      "  → Saved new best SB drift model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SB Epoch 11 [train]: 100%|██████████| 31/31 [00:38<00:00,  1.24s/it]\n",
      "SB Epoch 11 [val]: 100%|██████████| 8/8 [00:11<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SB] Epoch 11: train=16398.6287, val=12842.7041\n",
      "  → Saved new best SB drift model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SB Epoch 12 [train]: 100%|██████████| 31/31 [00:37<00:00,  1.22s/it]\n",
      "SB Epoch 12 [val]: 100%|██████████| 8/8 [00:11<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SB] Epoch 12: train=15992.5994, val=13071.2753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SB Epoch 13 [train]: 100%|██████████| 31/31 [00:37<00:00,  1.22s/it]\n",
      "SB Epoch 13 [val]: 100%|██████████| 8/8 [00:11<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SB] Epoch 13: train=15065.7176, val=11231.6212\n",
      "  → Saved new best SB drift model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SB Epoch 14 [train]: 100%|██████████| 31/31 [00:37<00:00,  1.23s/it]\n",
      "SB Epoch 14 [val]: 100%|██████████| 8/8 [00:11<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SB] Epoch 14: train=14879.7699, val=11664.2050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SB Epoch 15 [train]: 100%|██████████| 31/31 [00:37<00:00,  1.22s/it]\n",
      "SB Epoch 15 [val]: 100%|██████████| 8/8 [00:11<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SB] Epoch 15: train=14603.0899, val=12149.1233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SB Epoch 16 [train]: 100%|██████████| 31/31 [00:37<00:00,  1.22s/it]\n",
      "SB Epoch 16 [val]: 100%|██████████| 8/8 [00:11<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SB] Epoch 16: train=13817.6266, val=10884.5660\n",
      "  → Saved new best SB drift model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SB Epoch 17 [train]: 100%|██████████| 31/31 [00:37<00:00,  1.22s/it]\n",
      "SB Epoch 17 [val]: 100%|██████████| 8/8 [00:11<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SB] Epoch 17: train=13523.8099, val=10163.7743\n",
      "  → Saved new best SB drift model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SB Epoch 18 [train]: 100%|██████████| 31/31 [00:37<00:00,  1.22s/it]\n",
      "SB Epoch 18 [val]: 100%|██████████| 8/8 [00:11<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SB] Epoch 18: train=13407.2477, val=11149.5535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SB Epoch 19 [train]: 100%|██████████| 31/31 [00:37<00:00,  1.22s/it]\n",
      "SB Epoch 19 [val]: 100%|██████████| 8/8 [00:11<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SB] Epoch 19: train=12860.7497, val=10000.7690\n",
      "  → Saved new best SB drift model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SB Epoch 20 [train]: 100%|██████████| 31/31 [00:37<00:00,  1.21s/it]\n",
      "SB Epoch 20 [val]: 100%|██████████| 8/8 [00:11<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SB] Epoch 20: train=12781.8149, val=10529.9153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SB Epoch 21 [train]: 100%|██████████| 31/31 [00:37<00:00,  1.22s/it]\n",
      "SB Epoch 21 [val]: 100%|██████████| 8/8 [00:11<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SB] Epoch 21: train=12623.5253, val=10527.3023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SB Epoch 22 [train]: 100%|██████████| 31/31 [00:37<00:00,  1.22s/it]\n",
      "SB Epoch 22 [val]: 100%|██████████| 8/8 [00:11<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SB] Epoch 22: train=12484.0485, val=10045.8189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SB Epoch 23 [train]: 100%|██████████| 31/31 [00:37<00:00,  1.21s/it]\n",
      "SB Epoch 23 [val]: 100%|██████████| 8/8 [00:11<00:00,  1.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SB] Epoch 23: train=12231.9453, val=9945.3959\n",
      "  → Saved new best SB drift model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SB Epoch 24 [train]: 100%|██████████| 31/31 [00:37<00:00,  1.22s/it]\n",
      "SB Epoch 24 [val]: 100%|██████████| 8/8 [00:11<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SB] Epoch 24: train=12367.2794, val=9616.0076\n",
      "  → Saved new best SB drift model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SB Epoch 25 [train]: 100%|██████████| 31/31 [00:37<00:00,  1.22s/it]\n",
      "SB Epoch 25 [val]: 100%|██████████| 8/8 [00:11<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SB] Epoch 25: train=11975.8909, val=9640.8428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SB Epoch 26 [train]: 100%|██████████| 31/31 [00:37<00:00,  1.22s/it]\n",
      "SB Epoch 26 [val]: 100%|██████████| 8/8 [00:11<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SB] Epoch 26: train=12139.4252, val=9602.4448\n",
      "  → Saved new best SB drift model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SB Epoch 27 [train]: 100%|██████████| 31/31 [00:37<00:00,  1.22s/it]\n",
      "SB Epoch 27 [val]: 100%|██████████| 8/8 [00:11<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SB] Epoch 27: train=11234.7088, val=8528.2705\n",
      "  → Saved new best SB drift model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SB Epoch 28 [train]: 100%|██████████| 31/31 [00:37<00:00,  1.22s/it]\n",
      "SB Epoch 28 [val]: 100%|██████████| 8/8 [00:11<00:00,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SB] Epoch 28: train=11399.9847, val=9406.2609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SB Epoch 29 [train]: 100%|██████████| 31/31 [00:37<00:00,  1.22s/it]\n",
      "SB Epoch 29 [val]: 100%|██████████| 8/8 [00:11<00:00,  1.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SB] Epoch 29: train=11330.8453, val=8254.4481\n",
      "  → Saved new best SB drift model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SB Epoch 30 [train]: 100%|██████████| 31/31 [00:37<00:00,  1.21s/it]\n",
      "SB Epoch 30 [val]: 100%|██████████| 8/8 [00:11<00:00,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SB] Epoch 30: train=10837.0811, val=9432.7264\n",
      "[SB] Training complete. Best val loss: 8254.448059082031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# 6. Main: SB training + evaluation\n",
    "# ===========================================\n",
    "def main():\n",
    "    # -------- Paths --------\n",
    "    rest_dir = \"/nfs/turbo/sph-jianghui1/yutingd/HCP/fALFF/\"\n",
    "    task_dir = \"/nfs/turbo/sph-jianghui1/yutingd/HCP/WM_Contrasts_2bk-0bk/\"\n",
    "    mask_path = \"/nfs/turbo/sph-jianghui1/yutingd/HCP/AAL_MNI_2mm.nii\"\n",
    "\n",
    "    train_ids = load_ids(\"/nfs/turbo/sph-jianghui1/yutingd/HCP/training_ids.txt\")\n",
    "    val_ids   = load_ids(\"/nfs/turbo/sph-jianghui1/yutingd/HCP/validation_ids.txt\")\n",
    "\n",
    "    test_ids  = load_ids(\"/nfs/turbo/sph-jianghui1/yutingd/HCP/test_ids.txt\")\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # -------- Load mask --------\n",
    "    mask_np = nib.load(mask_path).get_fdata()\n",
    "    mask_np = (mask_np > 0).astype(np.float32)\n",
    "    mask_t = torch.tensor(mask_np, dtype=torch.float32, device=device)[None, None]\n",
    "\n",
    "    # -------- Datasets & loaders --------\n",
    "    train_ds = HCPRestTaskDataset(rest_dir, task_dir, train_ids, mask_np=mask_np)\n",
    "    val_ds   = HCPRestTaskDataset(rest_dir, task_dir, val_ids, mask_np=mask_np)\n",
    "    test_ds  = HCPRestTaskDataset(rest_dir, task_dir, test_ids, mask_np=mask_np)\n",
    "    \n",
    "    B = 16\n",
    "    train_loader = DataLoader(train_ds, batch_size=B, shuffle=True, drop_last=True)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=B, shuffle=False, drop_last=True)\n",
    "    test_loader  = DataLoader(test_ds, batch_size=1, shuffle=False)\n",
    "\n",
    "    # -------- Load trained AE --------\n",
    "    latent_dim = 128\n",
    "    # -------- Load trained AE --------\n",
    "    encoder = Encoder3DStrong(latent_dim).to(device)\n",
    "\n",
    "    # run 1 sample through encoder to get feature_shape\n",
    "    rest, task = next(iter(train_loader))\n",
    "    #X = torch.cat([rest, task], dim=0).to(device)\n",
    "    X = task.to(device)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        z, feature_shape = encoder(X)\n",
    "    _, _, H0, W0, D0 = X.shape\n",
    "\n",
    "    decoder = Decoder3DStrong(\n",
    "        latent_dim,\n",
    "        feature_shape=feature_shape,\n",
    "        out_spatial_shape=(H0, W0, D0),\n",
    "    ).to(device)\n",
    "    encoder.load_state_dict(torch.load(\"encoder_ae_best.pth\", map_location=device))\n",
    "    decoder.load_state_dict(torch.load(\"decoder_ae_best.pth\", map_location=device))\n",
    "\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    for p in encoder.parameters():\n",
    "        p.requires_grad = False\n",
    "    for p in decoder.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    print(\"[SB] Loaded pretrained AE.\")\n",
    "\n",
    "    # -------- Drift network for SB --------\n",
    "    drift_net = DriftNet(latent_dim=latent_dim, hidden=256).to(device)\n",
    "    optimizer = torch.optim.Adam(drift_net.parameters(), lr=1e-4)\n",
    "    #scaler = GradScaler()\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    epochs_sb = 30\n",
    "    sigma = 0.3\n",
    "    best_val = float(\"inf\")\n",
    "    \n",
    "    stats = torch.load(\"latent_stats.pt\")\n",
    "    z_mean = stats[\"mean\"].to(device)\n",
    "    z_std  = stats[\"std\"].to(device)\n",
    "\n",
    "\n",
    "    # -------- SB training (latent flow-matching) --------\n",
    "    for epoch in range(1, epochs_sb + 1):\n",
    "        drift_net.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for rest, task in tqdm(train_loader, desc=f\"SB Epoch {epoch} [train]\"):\n",
    "            rest = rest.to(device)   # (1,1,H,W,D)\n",
    "            task = task.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # ---- raw latents ----\n",
    "                z_r, _ = encoder(rest)\n",
    "                z_t, _ = encoder(task)\n",
    "\n",
    "                # ---- normalize latents ----\n",
    "                z_r_n = (z_r - z_mean) / z_std\n",
    "                z_t_n = (z_t - z_mean) / z_std\n",
    "\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            with autocast():\n",
    "                #loss = flow_matching_loss(z_r, z_t, drift_net, sigma=sigma)\n",
    "                # FM core\n",
    "                fm_loss = flow_matching_loss(z_r_n, z_t_n,  drift_net, sigma=sigma)\n",
    "\n",
    "                # predicted midpoint latent in normalized space\n",
    "                z_mid_n = (z_r_n + z_t_n) / 2\n",
    "\n",
    "                # unnormalize\n",
    "                z_mid = z_mid_n * z_std + z_mean\n",
    "                z_t_un = z_t_n * z_std + z_mean\n",
    "\n",
    "                # decode\n",
    "                x_mid = decoder(z_mid, rest.shape)\n",
    "                X_pred = decoder(z_t_un, task.shape)\n",
    "\n",
    "                # ===========================\n",
    "                # 3) IMAGE-SPACE LOSSES\n",
    "                # ===========================\n",
    "                task_loss = masked_mse(X_pred, task, mask_t)\n",
    "                recon_loss = masked_mse(x_mid, task, mask_t)\n",
    "\n",
    "                scale_loss = torch.mean(x_mid**2)\n",
    "\n",
    "                loss = fm_loss + task_loss + 0.1 * recon_loss + 1e-4 * scale_loss\n",
    "\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(drift_net.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        # ---- validation: use flow-matching loss on val set ----\n",
    "        drift_net.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for rest, task in tqdm(val_loader, desc=f\"SB Epoch {epoch} [val]\"):\n",
    "                rest = rest.to(device)\n",
    "                task = task.to(device)\n",
    "\n",
    "                z_r, _ = encoder(rest)\n",
    "                z_t, _ = encoder(task)\n",
    "\n",
    "                z_r_n = (z_r - z_mean) / z_std\n",
    "                z_t_n = (z_t - z_mean) / z_std\n",
    "\n",
    "                fm_loss = flow_matching_loss(z_r_n, z_t_n, drift_net, sigma=sigma)\n",
    "\n",
    "                z_mid_n = (z_r_n + z_t_n) / 2\n",
    "                z_mid = z_mid_n * z_std + z_mean\n",
    "                x_mid = decoder(z_mid, rest.shape)\n",
    "\n",
    "                z_t_un = z_t_n * z_std + z_mean\n",
    "                X_pred = decoder(z_t_un, task.shape)\n",
    "\n",
    "                recon_loss = masked_mse(x_mid, task, mask_t)\n",
    "                task_loss  = masked_mse(X_pred, task, mask_t)\n",
    "                scale_loss = torch.mean(x_mid**2)\n",
    "\n",
    "                loss = fm_loss + task_loss + 0.1 * recon_loss + 1e-4 * scale_loss\n",
    "\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        print(f\"[SB] Epoch {epoch}: train={train_loss:.4f}, val={val_loss:.4f}\")\n",
    "\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            torch.save(drift_net.state_dict(), \"driftnet_sb_best.pth\")\n",
    "            print(\"  → Saved new best SB drift model\")\n",
    "\n",
    "    print(\"[SB] Training complete. Best val loss:\", best_val)\n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "9f94f8d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[140]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m z0, _ = \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m z0 = z0.to(device)\n\u001b[32m      3\u001b[39m z_mean = z_mean.to(device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[111]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mEncoder3DStrong.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     36\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[33;03m    x: (B,1,H,W,D)\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[33;03m    returns:\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[33;03m      z: (B, latent_dim)\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[33;03m      feature_shape: (C, H', W', D') needed by decoder\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     h = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m          \u001b[38;5;66;03m# (B, C, H, W, D)\u001b[39;00m\n\u001b[32m     43\u001b[39m     h = \u001b[38;5;28mself\u001b[39m.down1(h)         \u001b[38;5;66;03m# (B, 2C, H/2, W/2, D/2)\u001b[39;00m\n\u001b[32m     44\u001b[39m     h = \u001b[38;5;28mself\u001b[39m.down2(h)         \u001b[38;5;66;03m# (B, 4C, ...)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/nn/modules/conv.py:725\u001b[39m, in \u001b[36mConv3d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    724\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m725\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/nn/modules/conv.py:720\u001b[39m, in \u001b[36mConv3d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    708\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    709\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv3d(\n\u001b[32m    710\u001b[39m         F.pad(\n\u001b[32m    711\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    718\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    719\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv3d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    721\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    722\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"
     ]
    }
   ],
   "source": [
    "z0, _ = encoder(rest)\n",
    "z0 = z0.to(device)\n",
    "z_mean = z_mean.to(device)\n",
    "z_std = z_std.to(device)\n",
    "z0_n = (z0 - z_mean) / z_std\n",
    "z0_u = z0_n * z_std + z_mean\n",
    "X_test = decoder(z0_u, rest.shape)\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e7cf4b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2b47cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_corr(X_hat, X, mask_t, eps=1e-6):\n",
    "    \"\"\"\n",
    "    Compute voxelwise correlation only in masked region.\n",
    "    Works for non-contiguous tensors.\n",
    "    \"\"\"\n",
    "    mask_flat = mask_t.reshape(-1) > 0\n",
    "    x = X_hat.reshape(-1)[mask_flat]\n",
    "    y = X.reshape(-1)[mask_flat]\n",
    "\n",
    "    if x.numel() < 2:\n",
    "        return torch.tensor(0.0, device=X.device)\n",
    "\n",
    "    x = x - x.mean()\n",
    "    y = y - y.mean()\n",
    "\n",
    "    num = (x * y).sum()\n",
    "    den = torch.sqrt((x**2).sum() * (y**2).sum() + eps)\n",
    "    return num / den\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10f711d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def gaussian_kernel_3d(channels, kernel_size=11, sigma=1.5):\n",
    "    coords = torch.arange(kernel_size) - kernel_size // 2\n",
    "    g = torch.exp(-(coords**2) / (2 * sigma**2))\n",
    "    g = g / g.sum()\n",
    "    kernel = g[:,None,None] * g[None,:,None] * g[None,None,:]\n",
    "    kernel = kernel[None,None,:,:,:]\n",
    "    kernel = kernel.repeat(channels, 1, 1, 1, 1)\n",
    "    return kernel\n",
    "\n",
    "def masked_ssim_3d(pred, true, mask, C1=0.01**2, C2=0.03**2):\n",
    "    \"\"\"\n",
    "    pred, true, mask: shape (1,1,H,W,D)\n",
    "    \"\"\"\n",
    "    kernel = gaussian_kernel_3d(1).to(pred.device)\n",
    "\n",
    "    mu_pred = F.conv3d(pred, kernel, padding=5)\n",
    "    mu_true = F.conv3d(true, kernel, padding=5)\n",
    "\n",
    "    mu_pred_sq = mu_pred ** 2\n",
    "    mu_true_sq = mu_true ** 2\n",
    "    mu_pred_true = mu_pred * mu_true\n",
    "\n",
    "    sigma_pred = F.conv3d(pred * pred, kernel, padding=5) - mu_pred_sq\n",
    "    sigma_true = F.conv3d(true * true, kernel, padding=5) - mu_true_sq\n",
    "    sigma_pred_true = F.conv3d(pred * true, kernel, padding=5) - mu_pred_true\n",
    "\n",
    "    ssim_num = (2 * mu_pred_true + C1) * (2 * sigma_pred_true + C2)\n",
    "    ssim_den = (mu_pred_sq + mu_true_sq + C1) * (sigma_pred + sigma_true + C2)\n",
    "\n",
    "    ssim = ssim_num / (ssim_den + 1e-6)\n",
    "\n",
    "    # apply brain mask\n",
    "    masked_ssim = ssim[mask > 0].mean()\n",
    "    return masked_ssim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2b1a53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import scipy\n",
    "def get_features(dataloader, encoder, device):\n",
    "    feats = []\n",
    "\n",
    "    encoder.eval()\n",
    "    with torch.no_grad():\n",
    "        for rest, task in tqdm(dataloader):\n",
    "            task = task.to(device)\n",
    "            z = encoder(task)[0]  # shape (B, latent_dim)\n",
    "            feats.append(z.cpu().numpy())\n",
    "\n",
    "    feats = np.concatenate(feats, axis=0)\n",
    "    return feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0898675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fid(feats1, feats2):\n",
    "    mu1, mu2 = feats1.mean(0), feats2.mean(0)\n",
    "    cov1 = np.cov(feats1, rowvar=False)\n",
    "    cov2 = np.cov(feats2, rowvar=False)\n",
    "\n",
    "    diff = mu1 - mu2\n",
    "    covmean = scipy.linalg.sqrtm(cov1 @ cov2)\n",
    "\n",
    "    fid = diff.dot(diff) + np.trace(cov1 + cov2 - 2 * covmean)\n",
    "    return np.real(fid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b01b99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25c66512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_3d_slices(X, title, save_path):\n",
    "    \"\"\"\n",
    "    X: numpy array (H,W,D)\n",
    "    Plots axial, sagittal, and coronal slices.\n",
    "    \"\"\"\n",
    "    H, W, D = X.shape\n",
    "    slices = [\n",
    "        X[:, :, D // 2],   # axial (z mid)\n",
    "        X[:, W // 2, :],   # sagittal (y mid)\n",
    "        X[H // 2, :, :],   # coronal (x mid)\n",
    "    ]\n",
    "    slice_titles = [\"Axial (Z)\", \"Sagittal (Y)\", \"Coronal (X)\"]\n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "    for i, sl in enumerate(slices):\n",
    "        im = axs[i].imshow(sl.T, cmap=\"jet\", origin=\"lower\")\n",
    "        axs[i].set_title(slice_titles[i])\n",
    "        plt.colorbar(im, ax=axs[i], fraction=0.046)\n",
    "    fig.suptitle(title)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(save_path)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1aec699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram_comparison(true_img, pred_img, save_path):\n",
    "    \"\"\"\n",
    "    true_img, pred_img: numpy arrays (H,W,D)\n",
    "    \"\"\"\n",
    "    true_vals = true_img.flatten()\n",
    "    pred_vals = pred_img.flatten()\n",
    "    diff_vals = pred_vals - true_vals\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    plt.hist(true_vals, bins=80, alpha=0.5, label=\"True\", density=True)\n",
    "    plt.hist(pred_vals, bins=80, alpha=0.5, label=\"Predicted\", density=True)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.title(\"Histogram Comparison\")\n",
    "    plt.xlabel(\"Intensity\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.savefig(save_path.replace(\".png\", \"_hist.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Difference histogram\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.hist(diff_vals, bins=80, alpha=0.7, color=\"purple\")\n",
    "    plt.title(\"Histogram of Prediction Error (Pred - True)\")\n",
    "    plt.xlabel(\"Difference\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.savefig(save_path.replace(\".png\", \"_diffhist.png\"))\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11eee467",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_intermediate_states(axials, titles, save_path):\n",
    "    \"\"\"\n",
    "    axials: list of 2D axial slices (H,W)\n",
    "    titles: list of strings\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    n = len(axials)\n",
    "    plt.figure(figsize=(3*n, 4))\n",
    "\n",
    "    for i in range(n):\n",
    "        plt.subplot(1, n, i+1)\n",
    "        plt.imshow(axials[i], cmap=\"gray\")\n",
    "        plt.title(titles[i])\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=150)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4609cae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dataset] Loaded 73 subjects\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3148339/2345917193.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  encoder.load_state_dict(torch.load(\"encoder_ae_best.pth\", map_location=device))\n",
      "/tmp/ipykernel_3148339/2345917193.py:55: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  decoder.load_state_dict(torch.load(\"decoder_ae_best.pth\", map_location=device))\n",
      "/tmp/ipykernel_3148339/2345917193.py:69: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  drift_net.load_state_dict(torch.load(\"driftnet_sb_best.pth\", map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SB Test] Running predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/73 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'z_mean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 216\u001b[39m\n\u001b[32m    212\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[SB Test] Saved predictions to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m     \u001b[43mtest_and_save_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 91\u001b[39m, in \u001b[36mtest_and_save_predictions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# Encode rest → latent\u001b[39;00m\n\u001b[32m     90\u001b[39m z_r, _ = encoder(rest)\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m z_r_n = (z_r - \u001b[43mz_mean\u001b[49m) / z_std\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# Integrate latent ODE to get predicted latent task\u001b[39;00m\n\u001b[32m     96\u001b[39m z_pred_n = euler_integrate(z_r_n, drift_net, steps=\u001b[32m50\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'z_mean' is not defined"
     ]
    }
   ],
   "source": [
    "def test_and_save_predictions():\n",
    "\n",
    "    # ----------------------------\n",
    "    # 1. Paths (EDIT THESE)\n",
    "    # ----------------------------\n",
    "    rest_dir = \"/nfs/turbo/sph-jianghui1/yutingd/HCP/fALFF/\"\n",
    "    task_dir = \"/nfs/turbo/sph-jianghui1/yutingd/HCP/WM_Contrasts_2bk-0bk/\"\n",
    "    mask_path = \"/nfs/turbo/sph-jianghui1/yutingd/HCP/AAL_MNI_2mm.nii\"\n",
    "\n",
    "    \n",
    "    test_ids  = load_ids(\"/nfs/turbo/sph-jianghui1/yutingd/HCP/test_ids.txt\")\n",
    "\n",
    "    \n",
    "    save_dir = \"SB_predictions\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # ----------------------------\n",
    "    # 2. Load mask\n",
    "    # ----------------------------\n",
    "    mask_np = nib.load(mask_path).get_fdata()\n",
    "    mask_np = (mask_np > 0).astype(np.float32)\n",
    "    mask_t  = torch.tensor(mask_np, dtype=torch.float32)[None, None]\n",
    "\n",
    "    # ----------------------------\n",
    "    # 3. Dataset\n",
    "    # ----------------------------\n",
    "    test_ds = HCPRestTaskDataset(rest_dir, task_dir, test_ids, mask_np=mask_np)\n",
    "    test_loader = DataLoader(test_ds, batch_size=1, shuffle=False)\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # ----------------------------\n",
    "    # 4. Load trained AE\n",
    "    # ----------------------------\n",
    "    latent_dim = 128\n",
    "    # -------- Load trained AE --------\n",
    "    encoder = Encoder3DStrong(latent_dim).to(device)\n",
    "\n",
    "    # run 1 sample through encoder to get feature_shape\n",
    "    rest, task = next(iter(test_loader))\n",
    "    #X = torch.cat([rest, task], dim=0).to(device)\n",
    "    X = task.to(device)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        z, feature_shape = encoder(X)\n",
    "    _, _, H0, W0, D0 = X.shape\n",
    "\n",
    "    decoder = Decoder3DStrong(\n",
    "        latent_dim,\n",
    "        feature_shape=feature_shape,\n",
    "        out_spatial_shape=(H0, W0, D0),\n",
    "    ).to(device)\n",
    "    encoder.load_state_dict(torch.load(\"encoder_ae_best.pth\", map_location=device))\n",
    "    decoder.load_state_dict(torch.load(\"decoder_ae_best.pth\", map_location=device))\n",
    "\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    for p in encoder.parameters():\n",
    "        p.requires_grad = False\n",
    "    for p in decoder.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "    # ----------------------------\n",
    "    # 5. Load trained drift net\n",
    "    # ----------------------------\n",
    "    drift_net = DriftNet(latent_dim=latent_dim).to(device)\n",
    "    drift_net.load_state_dict(torch.load(\"driftnet_sb_best.pth\", map_location=device))\n",
    "    drift_net.eval()\n",
    "\n",
    "    # ----------------------------\n",
    "    # 6. Loop test set\n",
    "    # ----------------------------\n",
    "    mse_list, corr_list = [], []\n",
    "    ssim_list = []\n",
    "    fid_feats_true = []\n",
    "    fid_feats_pred = []\n",
    "\n",
    "\n",
    "    print(\"[SB Test] Running predictions...\")\n",
    "\n",
    "    for (rest, task), sid in tqdm(zip(test_loader, test_ids), total=len(test_ids)):\n",
    "\n",
    "        rest = rest.to(device)  # (1,1,H,W,D)\n",
    "        task = task.to(device)\n",
    "        H, W, D = rest.shape[2:5]\n",
    "\n",
    "        # Encode rest → latent\n",
    "        z_r, _ = encoder(rest)\n",
    "        z_r_n = (z_r - z_mean) / z_std\n",
    "\n",
    "\n",
    "\n",
    "        # Integrate latent ODE to get predicted latent task\n",
    "        z_pred_n = euler_integrate(z_r_n, drift_net, steps=50)\n",
    "        z_pred   = z_pred_n * z_std + z_mean\n",
    "        \n",
    "        # ---------------------------------------------\n",
    "        # Compute intermediate latent states manually\n",
    "        # ---------------------------------------------\n",
    "        t_values = [0.0, 0.25, 0.5, 0.75, 1.0]\n",
    "\n",
    "        z_states = []\n",
    "        for t in t_values:\n",
    "            t_tensor = torch.full((1,), t, device=device)\n",
    "            # one Euler step: predict drift at (z_r_n + t*(z_pred_n - z_r_n))\n",
    "            z_t = (1 - t) * z_r_n + t * z_pred_n\n",
    "            z_t = z_t * z_std + z_mean  # unstandardize\n",
    "            z_states.append(z_t)\n",
    "\n",
    "        # ---------------------------------------------\n",
    "        # Decode all 5 states\n",
    "        # ---------------------------------------------\n",
    "        decoded_states = []\n",
    "        for z in z_states:\n",
    "            X_t = decoder(z, rest.shape)\n",
    "            X_t = (X_t * mask_t_dev).squeeze().detach().cpu().numpy()\n",
    "            decoded_states.append(X_t)\n",
    "            \n",
    "        # axial slice visualization\n",
    "        mid_z = decoded_states[0].shape[2] // 2\n",
    "        axials = [x[:, :, mid_z] for x in decoded_states]\n",
    "        titles = [\"t=0\", \"t=0.25\", \"t=0.5\", \"t=0.75\", \"t=1\"]\n",
    "\n",
    "        interp_path = os.path.join(sub_dir, f\"{sid}_interp.png\")\n",
    "        visualize_intermediate_states(axials, titles, interp_path)\n",
    "        # -------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "        # Decode to full fMRI volume\n",
    "        X_pred = decoder(z_pred, rest.shape)\n",
    "        mask_t_dev = mask_t.to(device)\n",
    "        X_pred = X_pred * mask_t_dev   # zero-out outside brain\n",
    "        \n",
    "        X_true = task\n",
    "        \n",
    "        sub_dir = os.path.join(save_dir, \"diagnostics\", str(sid))\n",
    "        os.makedirs(sub_dir, exist_ok=True)\n",
    "\n",
    "        # Convert tensors to numpy arrays\n",
    "        pred_np = X_pred.squeeze().detach().cpu().numpy()\n",
    "        true_np = X_true.squeeze().detach().cpu().numpy()\n",
    "        err_np  = pred_np - true_np\n",
    "\n",
    "        # ---- 1. Histogram comparisons ----\n",
    "        plot_histogram_comparison(true_np, pred_np,\n",
    "                                  save_path=os.path.join(sub_dir, f\"{sid}_hist.png\"))\n",
    "        # ---- 2. Brain slices: prediction ----\n",
    "        visualize_3d_slices(\n",
    "            pred_np,\n",
    "            title=f\"Predicted ({sid})\",\n",
    "            save_path=os.path.join(sub_dir, f\"{sid}_pred.png\")\n",
    "        )\n",
    "\n",
    "        # ---- 3. Brain slices: true image ----\n",
    "        visualize_3d_slices(\n",
    "            true_np,\n",
    "            title=f\"True Task ({sid})\",\n",
    "            save_path=os.path.join(sub_dir, f\"{sid}_true.png\")\n",
    "        )\n",
    "\n",
    "        # ---- 4. Error map visualization ----\n",
    "        visualize_3d_slices(\n",
    "            err_np,\n",
    "            title=f\"Error Map (Pred - True) {sid}\",\n",
    "            save_path=os.path.join(sub_dir, f\"{sid}_error.png\")\n",
    "        )\n",
    "                # Compute masked metrics\n",
    "        mask_t_dev = mask_t.to(device)\n",
    "        mse = masked_mse(X_pred, X_true, mask_t_dev).item()\n",
    "        corr = masked_corr(X_pred, X_true, mask_t_dev).item()\n",
    "        ssim = masked_ssim_3d(X_pred, task, mask_t_dev).item()\n",
    "\n",
    "        mse_list.append(mse)\n",
    "        corr_list.append(corr)\n",
    "        ssim_list.append(ssim)\n",
    "        # latent FID features\n",
    "        with torch.no_grad():\n",
    "            z_true = encoder(task)[0].detach().cpu().numpy()\n",
    "            z_pred = encoder(X_pred)[0].detach().cpu().numpy()\n",
    "\n",
    "        fid_feats_true.append(z_true)\n",
    "        fid_feats_pred.append(z_pred)\n",
    "\n",
    "        # ---------------------------------------------\n",
    "        # 7. Save predicted NIfTI file\n",
    "        # ---------------------------------------------\n",
    "        pred_np = X_pred.squeeze().detach().cpu().numpy()  # (H,W,D)\n",
    "\n",
    "        # Load reference affine (keep spatial location correct)\n",
    "        ref_path = os.path.join(rest_dir, f\"{sid}_fALFF.nii\")\n",
    "        aff = nib.load(ref_path).affine\n",
    "\n",
    "        # Save predicted task image\n",
    "        save_path = os.path.join(save_dir, f\"{sid}_SBpred.nii\")\n",
    "        nib.save(nib.Nifti1Image(pred_np, affine=aff), save_path)\n",
    "\n",
    "\n",
    "    print(\"---- Test Summary ----\")\n",
    "    print(f\"Mean Masked MSE:  {np.mean(mse_list):.4f}\")\n",
    "    print(f\"Mean Masked Corr: {np.mean(corr_list):.4f}\")\n",
    "    print(f\"Mean Masked SSIM: {np.mean(ssim_list):.4f}\")\n",
    "    \n",
    "    fid_val = compute_fid(np.vstack(fid_feats_true),\n",
    "                      np.vstack(fid_feats_pred))\n",
    "    \n",
    "    print(f\"FID: {np.mean(fid_val):.4f}\")\n",
    "\n",
    "\n",
    "    print(f\"[SB Test] Saved predictions to {save_dir}/\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_and_save_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fe3e085b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task true  mean/std: tensor([[[[[ 1.3524e-09]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 2.9212e-08]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-1.9474e-08]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-5.6800e-09]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 6.2210e-09]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 1.7581e-08]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-3.2052e-08]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-7.8438e-09]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 1.9610e-08]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-8.1143e-09]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 2.1909e-08]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-1.7581e-08]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-3.0158e-08]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-1.7851e-08]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[-7.0324e-09]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[ 5.6800e-09]]]]], device='cuda:0') 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Task true  mean/std:\", task.mean(dim=[1,2,3,4], keepdim=True), task.std().item())\n",
    "#print(\"Task recon mean/std:\", X_pred.mean().item(), X_pred.std().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "002358f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Saved] sb_100307.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_three_views(\n",
    "    rest_nii,\n",
    "    true_task_nii,\n",
    "    pred_task_nii,\n",
    "    save_path=\"sb_vis.png\",\n",
    "    title=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot axial, sagittal, coronal slices for:\n",
    "        - rest\n",
    "        - true task\n",
    "        - predicted task\n",
    "\n",
    "    Inputs are file paths to NIfTI files.\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------\n",
    "    # Load volumes\n",
    "    # --------------------------\n",
    "    rest = nib.load(rest_nii).get_fdata()\n",
    "    true = nib.load(true_task_nii).get_fdata()\n",
    "    pred = nib.load(pred_task_nii).get_fdata()\n",
    "\n",
    "    # Ensure float32\n",
    "    rest = np.array(rest, dtype=np.float32)\n",
    "    true = np.array(true, dtype=np.float32)\n",
    "    pred = np.array(pred, dtype=np.float32)\n",
    "\n",
    "    # --------------------------\n",
    "    # Choose slices (middle slices)\n",
    "    # --------------------------\n",
    "    H, W, D = rest.shape\n",
    "\n",
    "    slice_x = H // 2   # sagittal\n",
    "    slice_y = W // 2   # coronal\n",
    "    slice_z = D // 2   # axial\n",
    "\n",
    "    # --------------------------\n",
    "    # Three views × 3 images = 9 panels\n",
    "    # --------------------------\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(10, 10))\n",
    "    plt.subplots_adjust(wspace=0.02, hspace=0.02)\n",
    "\n",
    "    # Helper to remove borders\n",
    "    def clean_axis(ax):\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    # --------------------------\n",
    "    # Row 1: AXIAL (z)\n",
    "    # --------------------------\n",
    "    axes[0, 0].imshow(rest[:, :, slice_z].T, cmap=\"gray\", origin=\"lower\")\n",
    "    axes[0, 1].imshow(true[:, :, slice_z].T, cmap=\"gray\", origin=\"lower\")\n",
    "    axes[0, 2].imshow(pred[:, :, slice_z].T, cmap=\"gray\", origin=\"lower\")\n",
    "    axes[0, 0].set_ylabel(\"Axial\", fontsize=12)\n",
    "\n",
    "    # --------------------------\n",
    "    # Row 2: SAGITTAL (x)\n",
    "    # --------------------------\n",
    "    axes[1, 0].imshow(rest[slice_x, :, :].T, cmap=\"gray\", origin=\"lower\")\n",
    "    axes[1, 1].imshow(true[slice_x, :, :].T, cmap=\"gray\", origin=\"lower\")\n",
    "    axes[1, 2].imshow(pred[slice_x, :, :].T, cmap=\"gray\", origin=\"lower\")\n",
    "    axes[1, 0].set_ylabel(\"Sagittal\", fontsize=12)\n",
    "\n",
    "    # --------------------------\n",
    "    # Row 3: CORONAL (y)\n",
    "    # --------------------------\n",
    "    axes[2, 0].imshow(rest[:, slice_y, :].T, cmap=\"gray\", origin=\"lower\")\n",
    "    axes[2, 1].imshow(true[:, slice_y, :].T, cmap=\"gray\", origin=\"lower\")\n",
    "    axes[2, 2].imshow(pred[:, slice_y, :].T, cmap=\"gray\", origin=\"lower\")\n",
    "    axes[2, 0].set_ylabel(\"Coronal\", fontsize=12)\n",
    "\n",
    "    # --------------------------\n",
    "    # Column Titles\n",
    "    # --------------------------\n",
    "    axes[0, 0].set_title(\"Rest\", fontsize=14)\n",
    "    axes[0, 1].set_title(\"True Task\", fontsize=14)\n",
    "    axes[0, 2].set_title(\"SB Predicted\", fontsize=14)\n",
    "\n",
    "    # Clean ticks\n",
    "    for ax_row in axes:\n",
    "        for ax in ax_row:\n",
    "            clean_axis(ax)\n",
    "\n",
    "    if title is not None:\n",
    "        fig.suptitle(title, fontsize=16)\n",
    "\n",
    "    # Save image\n",
    "    fig.savefig(save_path, dpi=200, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"[Saved] {save_path}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Example usage\n",
    "    subject = \"100307\"\n",
    "\n",
    "    rest_nii = f\"/nfs/turbo/sph-jianghui1/yutingd/HCP/fALFF/{subject}_fALFF.nii\"\n",
    "    true_task_nii = f\"/nfs/turbo/sph-jianghui1/yutingd/HCP/WM_Contrasts_2bk-0bk/{subject}_2bk-0bk.nii\"\n",
    "    pred_task_nii = f\"SB_predictions/{subject}_SBpred.nii\"\n",
    "\n",
    "    plot_three_views(\n",
    "        rest_nii, true_task_nii, pred_task_nii,\n",
    "        save_path=f\"sb_{subject}.png\",\n",
    "        title=f\"Subject {subject}: Rest vs Task vs SB Prediction\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "13c975a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['149842',\n",
       " '620434',\n",
       " '300618',\n",
       " '456346',\n",
       " '131924',\n",
       " '638049',\n",
       " '283543',\n",
       " '100307',\n",
       " '885975',\n",
       " '628248',\n",
       " '126426',\n",
       " '162329',\n",
       " '111009',\n",
       " '536647',\n",
       " '353740',\n",
       " '395958',\n",
       " '220721',\n",
       " '129129',\n",
       " '207426',\n",
       " '635245',\n",
       " '209935',\n",
       " '173940',\n",
       " '103414',\n",
       " '200008',\n",
       " '727553',\n",
       " '274542',\n",
       " '186141',\n",
       " '151627',\n",
       " '987074',\n",
       " '835657',\n",
       " '481042',\n",
       " '955465',\n",
       " '524135',\n",
       " '174841',\n",
       " '645450',\n",
       " '633847',\n",
       " '157942',\n",
       " '453441',\n",
       " '541943',\n",
       " '118730',\n",
       " '449753',\n",
       " '627852',\n",
       " '177645',\n",
       " '105923',\n",
       " '199453',\n",
       " '412528',\n",
       " '149539',\n",
       " '186848',\n",
       " '158136',\n",
       " '268749',\n",
       " '325129',\n",
       " '506234',\n",
       " '209228',\n",
       " '198249',\n",
       " '873968',\n",
       " '139435',\n",
       " '878776',\n",
       " '130720',\n",
       " '238033',\n",
       " '128127',\n",
       " '406836',\n",
       " '205119',\n",
       " '165638',\n",
       " '424939',\n",
       " '136833',\n",
       " '756055',\n",
       " '105620',\n",
       " '965771',\n",
       " '123723',\n",
       " '132017',\n",
       " '185442',\n",
       " '371843',\n",
       " '131217']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cdc1cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miniconda:myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
